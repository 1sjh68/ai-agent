# dev_toolkit.py

import os
import sys
import logging
import asyncio
import argparse
from datetime import datetime

# --- è·¯å¾„ä¿®æ­£ä»£ç  ---
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- æ ¸å¿ƒæ¨¡å—å¯¼å…¥ ---
from config import Config
from services.vector_db import EmbeddingModel, VectorDBManager
from utils.file_handler import load_external_data, parse_and_validate_paths

# [æ ¸å¿ƒä¿®æ­£] ä»æ­£ç¡®çš„ä½ç½®å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°
from utils.text_processor import (
    chunk_document_for_rag,
    consolidate_document_structure,
    final_post_processing,
    quality_check
)
from workflows.sub_workflows.polishing import perform_final_polish
from workflows.sub_workflows.planning import generate_style_guide

from planning.outline import generate_document_outline_with_tools
from services.web_research import run_research_cycle_async


# --- åˆå§‹åŒ– ---
config = Config()
config.setup_logging()

# --- å¼‚æ­¥å‡½æ•°è¿è¡Œå™¨ ---
def run_async(coro):
    """
    ä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨ï¼Œç”¨äºåœ¨åŒæ­¥ä»£ç ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°ã€‚
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    # nest_asyncio.apply() might be needed in some environments like Jupyter
    return loop.run_until_complete(coro)

# --- åŠŸèƒ½å®ç° ---

def seed_knowledge_base(args):
    """åŠŸèƒ½ä¸€ï¼šå°†ä¸ªäººæ•°æ®å­˜å…¥å‘é‡æ•°æ®åº“"""
    logging.info("--- å¼€å‘è€…æ¨¡å¼ï¼šå¯åŠ¨çŸ¥è¯†åº“æ„å»ºå·¥å…· ---")
    valid_files = parse_and_validate_paths(args.files)
    if not valid_files:
        logging.error("æœªèƒ½æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶ã€‚æ“ä½œç»ˆæ­¢ã€‚")
        return

    try:
        logging.info("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’Œå‘é‡æ•°æ®åº“...")
        embedding_model_instance = EmbeddingModel(config)
        db_manager = VectorDBManager(config, embedding_model_instance)
        logging.info("åˆå§‹åŒ–å®Œæˆã€‚")

        doc_id = args.doc_id or f"custom_seed_{datetime.now().strftime('%Y%m%d')}"
        
        logging.info(f"æ­£åœ¨åŠ è½½å’Œå¤„ç† {len(valid_files)} ä¸ªæ–‡ä»¶...")
        full_content = load_external_data(config, valid_files)
        
        if not full_content:
            logging.error("åŠ è½½çš„æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚æ“ä½œç»ˆæ­¢ã€‚")
            return
            
        chunks, metadatas = chunk_document_for_rag(config, full_content, doc_id)
        
        if not chunks:
            logging.error("æœªèƒ½ä»æ–‡ä»¶ä¸­åˆ‡åˆ†å‡ºä»»ä½•çŸ¥è¯†å—ã€‚æ“ä½œç»ˆæ­¢ã€‚")
            return

        logging.info(f"æˆåŠŸåˆ‡åˆ†å‡º {len(chunks)} ä¸ªçŸ¥è¯†å—ï¼Œå‡†å¤‡å­˜å…¥æ•°æ®åº“...")
        success = db_manager.add_experience(texts=chunks, metadatas=metadatas)
        
        if success:
            logging.info(f"ğŸ‰ æˆåŠŸå°† {len(chunks)} ä¸ªçŸ¥è¯†å—ä» {len(valid_files)} ä¸ªæ–‡ä»¶ä¸­æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼")
        else:
            logging.error("å­˜å…¥æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯ã€‚")

    except Exception as e:
        logging.critical(f"æ‰§è¡ŒçŸ¥è¯†åº“æ„å»ºæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)

def polish_document(args):
    """åŠŸèƒ½äºŒï¼šä¿®å¤ä¸æ¶¦è‰²ç°æœ‰æ–‡æ¡£"""
    logging.info("--- å¼€å‘è€…æ¨¡å¼ï¼šå¯åŠ¨ç‹¬ç«‹æ–‡æ¡£æ¶¦è‰²å·¥å…· ---")
    
    if not os.path.exists(args.input):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
        
    try:
        logging.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {args.input}")
        with open(args.input, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä¸ºäº†æ¶¦è‰²ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé£æ ¼æŒ‡å—ã€‚è¿™é‡Œæˆ‘ä»¬å³æ—¶ç”Ÿæˆä¸€ä¸ªã€‚
        logging.info("æ­£åœ¨ä¸ºæ–‡æ¡£ç”Ÿæˆä¸´æ—¶é£æ ¼æŒ‡å—...")
        temp_config = Config()
        temp_config.user_problem = f"å¯¹ä»¥ä¸‹æ–‡æ¡£è¿›è¡Œé«˜è´¨é‡çš„æ¶¦è‰²å’Œæ ¼å¼ä¿®å¤ï¼š\n\n{content[:500]}..."
        style_guide = generate_style_guide(temp_config)
        
        logging.info("æ­£åœ¨è¿›è¡ŒAIæ¶¦è‰²...")
        polished_content = perform_final_polish(config, content, style_guide)
        
        logging.info("æ­£åœ¨è¿›è¡Œç»“æ„æ•´åˆ...")
        structured_content = consolidate_document_structure(polished_content)
        
        logging.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ ¼å¼åå¤„ç†...")
        final_content = final_post_processing(structured_content)
        
        output_path = args.output or f"{os.path.splitext(args.input)[0]}_polished.md"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
            
        logging.info(f"ğŸ‰ æ¶¦è‰²å’Œä¿®å¤å®Œæˆï¼å·²ä¿å­˜è‡³: {output_path}")

    except Exception as e:
        logging.critical(f"æ‰§è¡Œæ–‡æ¡£æ¶¦è‰²æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)

def generate_outline(args):
    """å¦™ç”¨3ï¼šå¿«é€Ÿå¤§çº²ç”Ÿæˆå™¨"""
    logging.info("--- å¼€å‘è€…æ¨¡å¼ï¼šå¯åŠ¨å¿«é€Ÿå¤§çº²ç”Ÿæˆå™¨ ---")
    try:
        config._initialize_deepseek_client()
        outline_data = generate_document_outline_with_tools(config, args.prompt)
        if outline_data:
            output_path = args.output or "generated_outline.json"
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(outline_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ‰ å¤§çº²å·²ç”Ÿæˆï¼å·²ä¿å­˜è‡³: {output_path}")
        else:
            logging.error("ç”Ÿæˆå¤§çº²å¤±è´¥ã€‚")
    except Exception as e:
        logging.critical(f"æ‰§è¡Œå¤§çº²ç”Ÿæˆæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        
def research_gaps(args):
    """å¦™ç”¨4ï¼šAIç ”ç©¶åŠ©ç†"""
    logging.info("--- å¼€å‘è€…æ¨¡å¼ï¼šå¯åŠ¨AIç ”ç©¶åŠ©ç† ---")
    try:
        config._initialize_deepseek_client()
        research_brief = run_async(run_research_cycle_async(config, args.gaps, args.context or ""))
        if research_brief:
            output_path = "research_brief.txt"
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(research_brief)
            logging.info(f"ğŸ‰ ç ”ç©¶å®Œæˆï¼ç ”ç©¶ç®€æŠ¥å·²ä¿å­˜è‡³: {output_path}")
        else:
            logging.warning("æœªèƒ½ç”Ÿæˆä»»ä½•ç ”ç©¶ç®€æŠ¥ã€‚")
    except Exception as e:
        logging.critical(f"æ‰§è¡Œç ”ç©¶æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)

def assess_quality(args):
    """å¦™ç”¨5ï¼šæ–‡ç« è´¨é‡è¯„ä¼°å™¨"""
    logging.info("--- å¼€å‘è€…æ¨¡å¼ï¼šå¯åŠ¨æ–‡ç« è´¨é‡è¯„ä¼°å™¨ ---")
    if not os.path.exists(args.file):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    try:
        with open(args.file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        config._initialize_deepseek_client()
        report = quality_check(config, content)
        logging.info("\n--- è´¨é‡è¯„ä¼°æŠ¥å‘Š ---\n")
        print(report)
        logging.info("\n--- æŠ¥å‘Šç»“æŸ ---")
    except Exception as e:
        logging.critical(f"æ‰§è¡Œè´¨é‡è¯„ä¼°æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)


def main():
    parser = argparse.ArgumentParser(description="Deep Research - å¼€å‘è€…å·¥å…·ç®±")
    subparsers = parser.add_subparsers(dest="command", required=True, help="å¯ç”¨çš„å‘½ä»¤")

    # åŠŸèƒ½ä¸€ï¼šseed-db
    parser_seed = subparsers.add_parser("seed-db", help="å°†ä¸ªäººæ–‡ä»¶ä½œä¸ºçŸ¥è¯†å­˜å…¥å‘é‡æ•°æ®åº“ã€‚")
    parser_seed.add_argument("--files", required=True, type=str, help="è¦æ·»åŠ çš„æ–‡ä»¶è·¯å¾„ï¼Œç”¨é€—å·åˆ†éš”ã€‚")
    parser_seed.add_argument("--doc_id", type=str, help="ï¼ˆå¯é€‰ï¼‰ä¸ºè¿™æ‰¹æ–‡æ¡£æŒ‡å®šä¸€ä¸ªè‡ªå®šä¹‰IDã€‚")
    parser_seed.set_defaults(func=seed_knowledge_base)

    # åŠŸèƒ½äºŒï¼špolish
    parser_polish = subparsers.add_parser("polish", help="å¯¹ä¸€ä¸ªç°æœ‰çš„Markdownæ–‡ä»¶è¿›è¡Œä¿®å¤å’Œæ¶¦è‰²ã€‚")
    parser_polish.add_argument("--input", required=True, type=str, help="è¾“å…¥çš„Markdownæ–‡ä»¶è·¯å¾„ã€‚")
    parser_polish.add_argument("--output", type=str, help="ï¼ˆå¯é€‰ï¼‰è¾“å‡ºçš„æ–‡ä»¶è·¯å¾„ã€‚")
    parser_polish.set_defaults(func=polish_document)
    
    # å¦™ç”¨3ï¼šgenerate-outline
    parser_outline = subparsers.add_parser("generate-outline", help="æ ¹æ®ä¸€ä¸ªä¸»é¢˜å¿«é€Ÿç”Ÿæˆç»“æ„åŒ–å¤§çº²ã€‚")
    parser_outline.add_argument("--prompt", required=True, type=str, help="ç”Ÿæˆå¤§çº²çš„ä¸»é¢˜æˆ–æç¤ºã€‚")
    parser_outline.add_argument("--output", type=str, help="ï¼ˆå¯é€‰ï¼‰è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„ã€‚")
    parser_outline.set_defaults(func=generate_outline)
    
    # å¦™ç”¨4ï¼šresearch
    parser_research = subparsers.add_parser("research", help="é’ˆå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªçŸ¥è¯†ç©ºç™½è¿›è¡Œç½‘ç»œç ”ç©¶ã€‚")
    parser_research.add_argument("--gaps", required=True, nargs='+', help="éœ€è¦ç ”ç©¶çš„ä¸€ä¸ªæˆ–å¤šä¸ªçŸ¥è¯†ç©ºç™½ç‚¹ï¼ˆé—®é¢˜ï¼‰ã€‚")
    parser_research.add_argument("--context", type=str, help="ï¼ˆå¯é€‰ï¼‰æä¾›ä¸€äº›ä¸Šä¸‹æ–‡ä»¥å¸®åŠ©ç”Ÿæˆæ›´ç²¾ç¡®çš„æŸ¥è¯¢ã€‚")
    parser_research.set_defaults(func=research_gaps)

    # å¦™ç”¨5ï¼šassess-quality
    parser_assess = subparsers.add_parser("assess-quality", help="å¯¹ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶è¿›è¡ŒAIè´¨é‡è¯„ä¼°ã€‚")
    parser_assess.add_argument("--file", required=True, type=str, help="è¦è¯„ä¼°çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ã€‚")
    parser_assess.set_defaults(func=assess_quality)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()