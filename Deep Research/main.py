# main.py

import os
import sys
import logging
import asyncio
from datetime import datetime
import json
import nest_asyncio
import re

# --- è·¯å¾„ä¿®æ­£ä»£ç  ---
try:
    project_root = os.path.dirname(os.path.abspath(__file__))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
except NameError:
    project_root = os.getcwd()
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# --- æ ¸å¿ƒæ¨¡å—å¯¼å…¥ ---
from config import Config
from services.vector_db import EmbeddingModel, VectorDBManager
from workflows.graph_runner import run_graph_workflow
from utils.text_processor import (
    consolidate_document_structure,
    final_post_processing,
    quality_check,
    truncate_text_for_context
)
from planning.outline import generate_document_outline_with_tools
from services.llm_interaction import call_ai
from utils.file_handler import parse_and_validate_paths
from planning.outline import generate_document_outline_with_tools
from services.llm_interaction import call_ai

# --- Section ID æ³¨å…¥å‡½æ•° ---
def inject_section_ids_into_content(content: str, outline_data: dict) -> str:
    """
    å°†å¤§çº²ä¸­çš„ç« èŠ‚IDç²¾ç¡®æ³¨å…¥åˆ°é‡æ„åçš„Markdownæ–‡æœ¬ä¸­ï¼Œä¸ºåç»­çš„è¡¥ä¸åº”ç”¨åšå‡†å¤‡ã€‚
    """
    logging.info("--- æ­£åœ¨ä¸ºé‡æ„åçš„å†…å®¹æ³¨å…¥Section ID ---")
    id_map = {}

    def clean_and_simplify_title(text: str) -> str:
        """ä¸€ä¸ªæ›´å¼ºå¤§çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ·±åº¦æ¸…ç†å’Œç®€åŒ–æ ‡é¢˜ä»¥è¿›è¡ŒåŒ¹é…ã€‚"""
        # ç§»é™¤Markdownæ ‡è®°ï¼Œå¦‚`##`, `###`, `**`ç­‰
        text = re.sub(r'^[#\s]*', '', text).strip()
        text = re.sub(r'[#*`]', '', text).strip()
        # ç§»é™¤æ•°å­—å’Œç‚¹å·å‰ç¼€ï¼Œä¾‹å¦‚ "1. "
        text = re.sub(r'^\d+\.\s*', '', text).strip()
        # å°†å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_id_map(chapters):
        """é€’å½’æ„å»ºä¸€ä¸ªä»æ¸…ç†åçš„æ ‡é¢˜åˆ°IDçš„æ˜ å°„ã€‚"""
        for chapter in chapters:
            title = chapter.get("title", "").strip()
            clean_title_key = clean_and_simplify_title(title)
            if clean_title_key:
                id_map[clean_title_key] = chapter.get("id")
            if "sections" in chapter and chapter["sections"]:
                build_id_map(chapter["sections"])

    if "outline" in outline_data:
        build_id_map(outline_data["outline"])

    if not id_map:
        logging.error("æ— æ³•ä»å¤§çº²ä¸­æ„å»ºæ ‡é¢˜åˆ°IDçš„æ˜ å°„ã€‚IDæ³¨å…¥å¤±è´¥ã€‚")
        return content

    lines = content.split('\n')
    new_lines = []
    for line in lines:
        match = re.match(r'^(#+)\s*(.*)', line.strip())
        if match:
            heading_level_text = match.group(1)
            full_title_text = match.group(2).strip()
            
            cleaned_line_title = clean_and_simplify_title(full_title_text)
            
            section_id = id_map.get(cleaned_line_title)
            
            if section_id:
                # æ³¨å…¥IDï¼Œæ ¼å¼ä¸ºHTMLæ³¨é‡Š
                new_line = f"{heading_level_text} {full_title_text} "
                new_lines.append(new_line)
                logging.info(f"  - æˆåŠŸæ³¨å…¥ID for title: '{full_title_text}'")
            else:
                new_lines.append(line)
                logging.warning(f"  - æœªèƒ½ä¸ºæ ‡é¢˜ '{full_title_text}' (æ¸…ç†åä¸º: '{cleaned_line_title}') æ‰¾åˆ°åŒ¹é…çš„IDã€‚")
        else:
            new_lines.append(line)
            
    return "\n".join(new_lines)


async def main():
    config = Config()
    config.setup_logging()

    final_answer_processed = ""

    config.user_problem = os.getenv("USER_PROBLEM", "è¯·è¯¦ç»†é˜è¿°ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿ã€‚")
    external_files_str = os.getenv("EXTERNAL_FILES", "")
    config.external_data_files = parse_and_validate_paths(external_files_str)
    logging.info(f"ä»»åŠ¡é—®é¢˜ (å‰100å­—ç¬¦): {config.user_problem[:100]}...")

    try:
        config._initialize_deepseek_client()
    except Exception as e:
        logging.critical(f"è‡´å‘½é”™è¯¯ï¼šæ— æ³•åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯: {e}. ç¨‹åºå³å°†é€€å‡ºã€‚")
        sys.exit(1)

    vector_db_manager_instance = None
    try:
        embedding_model_instance = EmbeddingModel(config)
        if embedding_model_instance and embedding_model_instance.client:
            vector_db_manager_instance = VectorDBManager(config, embedding_model_instance)
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–åµŒå…¥æˆ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨æ—¶å‡ºé”™: {e}ã€‚åŠŸèƒ½å°†å—é™ã€‚", exc_info=True)

    initial_solution_from_file = None
    # --- [æ ¸å¿ƒä¿®æ”¹] å‡çº§ä¸ºç”±å…¨å±€å¤§çº²é©±åŠ¨çš„åˆ†å—é‡å†™æœºåˆ¶ ---
    if config.replay_from_file and os.path.exists(config.replay_from_file):
        logging.info(f"--- [æ™ºèƒ½é‡æ„æ¨¡å¼] å·²å¯ç”¨ (æ”¯æŒé•¿æ–‡æœ¬åˆ†å—) ---")
        logging.info(f"--- æ­£åœ¨ä» '{config.replay_from_file}' åŠ è½½å†…å®¹è¿›è¡Œç»“æ„ä¼˜åŒ– ---")
        try:
            with open(config.replay_from_file, "r", encoding="utf-8") as f:
                original_content = f.read()

            if not original_content.strip():
                raise ValueError("è¾“å…¥æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚")

            # æ­¥éª¤1: AIè§„åˆ’å¸ˆ - ä¸ºå…¨æ–‡ç”Ÿæˆä¸€ä¸ªå…¨å±€çš„ã€ç†æƒ³çš„å¤§çº²
            logging.info("æ­¥éª¤1: æ­£åœ¨ä¸ºå…¨æ–‡ç”Ÿæˆä¸€ä¸ªå…¨å±€çš„ç»“æ„åŒ–å¤§çº²...")
            outline_prompt = f"è¯·ä»”ç»†åˆ†æä»¥ä¸‹æä¾›çš„å®Œæ•´æ–‡æœ¬æ–‡æ¡£ï¼Œå¹¶ä¸ºå…¶åˆ›å»ºä¸€ä¸ªé€»è¾‘æ¸…æ™°ã€ç»“æ„åˆç†çš„æ–‡æ¡£å¤§çº²ã€‚å¤§çº²åº”è¯¥èƒ½æœ€å¥½åœ°ç»„ç»‡å’Œå‘ˆç°æ–‡ä¸­çš„ä¿¡æ¯ã€‚åŸå§‹æ–‡æœ¬å¦‚ä¸‹ï¼š\n\n---\n{truncate_text_for_context(config, original_content, 15000)}\n---"
            outline_data = generate_document_outline_with_tools(config, outline_prompt)
            
            if not outline_data or "outline" not in outline_data:
                raise RuntimeError("æœªèƒ½ä¸ºè¾“å…¥æ–‡æœ¬ç”Ÿæˆæœ‰æ•ˆå¤§çº²ï¼Œæ“ä½œç»ˆæ­¢ã€‚")
            
            logging.info(f"æˆåŠŸç”Ÿæˆå…¨å±€å¤§çº²ï¼Œå…± {len(outline_data.get('outline', []))} ä¸ªä¸»ç« èŠ‚ã€‚")

            # æ­¥éª¤2: AIç¼–è¾‘ - éå†æ–°å¤§çº²ï¼Œé€ä¸ªç« èŠ‚è¿›è¡Œå†…å®¹å¡«å……
            logging.info("æ­¥éª¤2: å¼€å§‹æ ¹æ®æ–°å¤§çº²ï¼Œé€ä¸ªç« èŠ‚è¿›è¡Œçº¦æŸæ€§é‡å†™...")
            
            truncated_original_content = truncate_text_for_context(config, original_content, 30000, "middle")
            restructured_parts = [f"# {outline_data.get('title', 'é‡æ„åçš„æ–‡æ¡£')}"]

            chapters_to_process = outline_data.get("outline", [])
            for i, chapter in enumerate(chapters_to_process):
                chapter_title = chapter.get("title", f"æœªå‘½åç« èŠ‚ {i+1}")
                logging.info(f"  -> æ­£åœ¨é‡å†™ç« èŠ‚ {i+1}/{len(chapters_to_process)}: '{chapter_title}'")
                
                rewrite_prompt = f"""
                ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æŠ€æœ¯ç¼–è¾‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„æ–°å¤§çº²ï¼Œé‡ç»„å’Œæ ¼å¼åŒ–æä¾›çš„åŸå§‹æ–‡æœ¬ã€‚

                **ä½ çš„è¡Œä¸ºå¿…é¡»éµå¾ªä»¥ä¸‹é“å¾‹ï¼š**
                1.  **å”¯ä¸€ä¿¡æº**ï¼šä½ åªèƒ½ä½¿ç”¨ã€åŸå§‹æ–‡æœ¬ã€‘ä¸­çš„å¥å­å’Œä¿¡æ¯ï¼Œç»ä¸å…è®¸è‡ªå·±åˆ›é€ ã€æœæ’°æˆ–å¼•å…¥ä»»ä½•å¤–éƒ¨ä¿¡æ¯ã€‚
                2.  **å†…å®¹å®Œæ•´**ï¼šä¸èƒ½é—æ¼ã€åŸå§‹æ–‡æœ¬ã€‘ä¸­çš„ä»»ä½•å…³é”®è®ºç‚¹ã€æ•°æ®æˆ–æ®µè½ã€‚ä½ çš„å·¥ä½œæ˜¯é‡æ–°ç»„ç»‡ï¼Œè€Œä¸æ˜¯åˆ å‡ã€‚
                3.  **èšç„¦ä»»åŠ¡**ï¼šåœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œä½ åªéœ€è¦æ’°å†™æ ‡é¢˜ä¸º **â€œ{chapter_title}â€** çš„è¿™ä¸€ä¸ªç« èŠ‚çš„å†…å®¹ã€‚
                4.  **æ ¼å¼è§„èŒƒ**ï¼šè¯·ä½¿ç”¨æ ‡å‡†çš„Markdownæ ¼å¼è¿›è¡Œè¾“å‡ºï¼Œå¹¶ä»¥æ­£ç¡®çš„æ ‡é¢˜çº§åˆ«ï¼ˆ`## {chapter_title}`ï¼‰å¼€å§‹ã€‚

                **ã€å®Œæ•´çš„æ–‡æ¡£å¤§çº² (ä¾›ä½ ç†è§£æ•´ä½“ç»“æ„)ã€‘**
                ```json
                {json.dumps(outline_data, ensure_ascii=False, indent=2)}
                ```

                **ã€åŸå§‹æ–‡æœ¬ (ä½ çš„å”¯ä¸€å†…å®¹æ¥æºï¼Œå¯èƒ½å·²è¢«æˆªæ–­)ã€‘**
                ```text
                {truncated_original_content}
                ```

                ç°åœ¨ï¼Œè¯·å¼€å§‹ä½ çš„å·¥ä½œï¼Œä»…è¾“å‡º **â€œ{chapter_title}â€** ç« èŠ‚çš„å®Œæ•´å†…å®¹ã€‚
                """
                messages = [{"role": "user", "content": rewrite_prompt}]
                
                chapter_content = call_ai(
                    config, 
                    config.main_ai_model_heavy, 
                    messages,
                    max_tokens_output=4096 
                )

                if "AIæ¨¡å‹è°ƒç”¨å¤±è´¥" in chapter_content or not chapter_content.strip():
                    logging.warning(f"    - ç« èŠ‚ '{chapter_title}' é‡å†™å¤±è´¥ï¼Œå°†è·³è¿‡ã€‚")
                    chapter_content = f"## {chapter_title}\n\n[å†…å®¹é‡æ„å¤±è´¥]\n\n"
                
                restructured_parts.append(chapter_content)

            # æ­¥éª¤3: [å…³é”®ä¿®å¤] å°†IDæ³¨å…¥åˆ°é‡æ„åçš„å†…å®¹ä¸­
            logging.info("æ­¥éª¤3: æ­£åœ¨å°†ç« èŠ‚IDæ³¨å…¥åˆ°é‡æ„åçš„å†…å®¹ä¸­ï¼Œä¸ºåç»­ä¼˜åŒ–åšå‡†å¤‡...")
            restructured_content_without_ids = "\n\n".join(restructured_parts)
            initial_solution_from_file = inject_section_ids_into_content(
                restructured_content_without_ids, 
                outline_data
            )

            logging.info(f"--- æ™ºèƒ½é‡æ„ä¸IDæ³¨å…¥å®Œæˆï¼Œå†…å®¹å°†ä½œä¸ºç¬¬0è½®åˆç¨¿è¿›å…¥ä¼˜åŒ–æµç¨‹ ---")

        except Exception as e:
            logging.critical(f"[æ™ºèƒ½é‡æ„æ¨¡å¼] é¢„å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return
    else:
        logging.info("--- [æ ‡å‡†æ¨¡å¼] å¯åŠ¨å®Œæ•´AIå†…å®¹åˆ›ä½œæ¡†æ¶ ---")

    # --- åç»­ä¸»æµç¨‹ä¿æŒä¸å˜ ---
    raw_final_result = "é”™è¯¯ï¼šç”±äºæœªçŸ¥åŸå› ï¼Œå·¥ä½œæµæœªèƒ½æˆåŠŸè¿è¡Œã€‚"
    try:
        raw_final_result = await run_graph_workflow(
            config,
            vector_db_manager_instance,
            initial_solution=initial_solution_from_file
        )

        if raw_final_result and not raw_final_result.startswith("é”™è¯¯ï¼š"):
            logging.info("\n--- å·¥ä½œæµå®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆçš„åå¤„ç†ã€è¯„ä¼°ä¸ä¿å­˜ ---")

            structured_answer = consolidate_document_structure(raw_final_result)
            final_answer_processed = final_post_processing(structured_answer)

            logging.info("\n--- æœ€ç»ˆäº§å‡ºè´¨é‡è¯„ä¼°æŠ¥å‘Š ---")
            quality_report = quality_check(config, final_answer_processed)
            logging.info(quality_report)
        else:
            final_answer_processed = raw_final_result

    except Exception as e:
        logging.critical(f"ä¸»å·¥ä½œæµç¨‹ 'generate_extended_content_workflow' å‘ç”Ÿæœªæ•è·çš„ä¸¥é‡å¼‚å¸¸: {e}", exc_info=True)
        final_answer_processed = f"é”™è¯¯ï¼šå·¥ä½œæµç¨‹å› ä¸¥é‡å¤±è´¥è€Œç»ˆæ­¢ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹æ—¥å¿—ã€‚ {e}"

    if final_answer_processed and not final_answer_processed.startswith("é”™è¯¯ï¼š"):
        try:
            prefix = "restructured_output" if config.replay_from_file else "final_solution"
            output_filename = f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            if config.session_dir and os.path.isdir(config.session_dir):
                output_filepath = os.path.join(config.session_dir, output_filename)
                with open(output_filepath, "w", encoding="utf-8") as f:
                    f.write(final_answer_processed)
                logging.info(f"ğŸ‰ æœ€ç»ˆæŠ¥å‘Šå·²æˆåŠŸä¿å­˜è‡³: {output_filepath}")
            else:
                logging.error("ä¼šè¯ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•ä¿å­˜æœ€ç»ˆæ–‡ä»¶ã€‚")
        except Exception as e:
            logging.error(f"ä¿å­˜æœ€ç»ˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        logging.error(f"è„šæœ¬æ‰§è¡Œç»“æŸï¼Œä½†æœ€ç»ˆç»“æœæ˜¯é”™è¯¯æˆ–ä¸ºç©º: {final_answer_processed}")


if __name__ == "__main__":
    nest_asyncio.apply()
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ã€‚")
    except Exception as e:
        logging.basicConfig(level=logging.INFO)
        logging.critical(f"åœ¨å¯åŠ¨æˆ–è¿è¡Œä¸»å¼‚æ­¥ä»»åŠ¡æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)